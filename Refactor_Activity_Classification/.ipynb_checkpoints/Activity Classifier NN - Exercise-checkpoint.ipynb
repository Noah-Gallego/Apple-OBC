{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN for Activity Classification\n",
    "---\n",
    "This notebook contains _incomplete_ code to train a neural net to predict an activityâ€”running or walkingâ€”based on some provided user motion data. \n",
    "\n",
    "### The data\n",
    "\n",
    "The data is a single csv file of accelerometer and gyroscope readings, collected from users' watches in 10 second intervals. Each data point is tagged as one of two activity types: running or walking. \n",
    "\n",
    "> Take a quick look at this data in Trove, where each column is described: [Run or walk data in Trove](https://trove.apple.com/dataset/run_walk_motion/1.0.0). \n",
    "\n",
    "### NN model creation\n",
    "\n",
    "Andy has started the process of creating and training a neural net, but it will be up to you to fix his code, and improve upon it! \n",
    "\n",
    "The **goal** of this notebook is that you gain experience defining neural nets in PyTorch code in multiple ways, and gain some intuition for what choices you can make to get better performing models.\n",
    "\n",
    "This notebook is broken up into the following NN model creation steps:\n",
    ">1. Load the data\n",
    "2. Create train/test dataloaders\n",
    "3. Define a neural network\n",
    "4. Train the model\n",
    "5. Evaluate the performance of our trained model on a test dataset\n",
    "6. Un-mount the Trove data\n",
    "7. UX Considerations\n",
    "\n",
    "Andy has gotten up to step 3, but his code to train a neural net is incomplete. Can you fix Andy's code, and create an NN model of your own that achieves better performance? \n",
    "\n",
    "As usual, your tasks will be marked as **TASKS** in markdown and as `## TODO's` in code. \n",
    "\n",
    "> **TASK**: Run the provided code to load the data and create train and test dataloaders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## run provided code\n",
    "\n",
    "# import PyTorch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "torch.manual_seed(0) # reproducibility\n",
    "\n",
    "# import data libraries\n",
    "import turitrove as trove\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Load the Data\n",
    "\n",
    "\n",
    "The following cells load run/walk activity data from Trove as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/run_walk_motion@1.0.0 is not mounted\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Local mount does not support spaces",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_data\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      8\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m activity_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtrove\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTROVE_URI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.8/site-packages/turitrove/_object_model/factory_methods.py:257\u001b[0m, in \u001b[0;36mmount\u001b[0;34m(trove_uri, local_dir, config, endpoint_only, tables_only, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m     local_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(local_dir))\n\u001b[0;32m--> 257\u001b[0m     api_response, mount_result, error \u001b[38;5;241m=\u001b[39m \u001b[43mdata_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmount\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrove_uri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrove_uri\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m res_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.8/site-packages/turitrove/_service/service_api/data_entity.py:93\u001b[0m, in \u001b[0;36mterms.<locals>.wrapper\u001b[0;34m(self, name, version, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, version, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_terms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.8/site-packages/turitrove/_service/service_api/data_entity.py:421\u001b[0m, in \u001b[0;36mDataEntityAPI._terms\u001b[0;34m(self, func, name, version, *args, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_terms\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, name, version, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_terms(name, version, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.8/site-packages/turitrove/_service/service_api/data_entity.py:1463\u001b[0m, in \u001b[0;36mDataEntityAPI.mount\u001b[0;34m(self, name, version, local_mount, local_cache, config, table_only, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;129m@terms\u001b[39m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmount\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, version, local_mount, local_cache, config, table_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;124;03m    use a data entity by mounting it to local_mount\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1459\u001b[0m \n\u001b[1;32m   1460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m     local_mount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_local_mount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_mount\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m     local_mount \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(local_mount)\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# for split, even table_only flag is set as True, unset table_only flag as False\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deep-learning/lib/python3.8/site-packages/turitrove/_service/service_api/data_entity.py:1404\u001b[0m, in \u001b[0;36mDataEntityAPI._validate_local_mount\u001b[0;34m(self, local_mount)\u001b[0m\n\u001b[1;32m   1402\u001b[0m local_mount_safe \u001b[38;5;241m=\u001b[39m local_mount\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(local_mount_safe) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m local_mount_safe\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1404\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocal mount does not support spaces\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_mount_safe\n",
      "\u001b[0;31mAttributeError\u001b[0m: Local mount does not support spaces"
     ]
    }
   ],
   "source": [
    "# load data from Trove\n",
    "\n",
    "TROVE_URI = 'dataset/run_walk_motion@1.0.0'\n",
    "trove.umount(TROVE_URI)\n",
    "\n",
    "# check for temp_data dir, if not found, make one and mount the dataset\n",
    "if not os.path.isdir('temp_data'):\n",
    "    os.makedirs('temp_data')\n",
    "\n",
    "activity_dataset = trove.mount(TROVE_URI, 'temp_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe from csv path\n",
    "\n",
    "data_path = activity_dataset.raw_file_path + '/'+ activity_dataset.primary_index['path'][0]\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data\n",
    "\n",
    "Andy has done some data exploration to see the amount of data and number of users in the provided data.\n",
    "\n",
    "> **TASK**: Write some code to answer the question: What percentage of this data is tagged as walking vs. running? \n",
    "\n",
    "The answer to this question should give you a good idea of whether you have _enough_ data for each class to make accurate predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data a bit\n",
    "\n",
    "# how much data?\n",
    "print('# rows: ', len(df))\n",
    "print()\n",
    "# how many users?\n",
    "print('Unique users: ', len(df['username'].unique()))\n",
    "# descriptive stats\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Find the distribution of run/walk data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format and save for model training\n",
    "\n",
    "This cell formats the DataFrame for model training; dropping non-featurized columns of data and moving the target to be the last column.\n",
    "\n",
    "Finally, the formatted data is saved in a local `data/` directory as a binary file `run_walk_formatted.pkl`, which can be read in again, later in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep for model training, format data\n",
    "\n",
    "# date, time, username columns dropped \n",
    "df_formatted = df.drop(['date', 'time', 'username'], axis=1)\n",
    "# put target (activity) column last\n",
    "column_order = ['wrist', 'acceleration_x', 'acceleration_y', 'acceleration_z', \n",
    "                'gyro_x', 'gyro_y', 'gyro_z','activity']\n",
    "df_formatted = df_formatted.reindex(columns=column_order)\n",
    "df_formatted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pkl file \n",
    "\n",
    "if not os.path.isdir('data'):\n",
    "    os.makedirs('data')\n",
    "    \n",
    "# save to local data/ dir\n",
    "df_formatted.to_pickle('data/run_walk_formatted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset\n",
    "\n",
    "The following code creates one dataset that loads in the formatted csv as **tensors**; where each sample of data holds input features and one, corresponding target `activity` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "\n",
    "from helpers import RunWalkDataset\n",
    "run_walk_dataset = RunWalkDataset('data/run_walk_formatted.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a few (3) samples to see that it looks right\n",
    "for i in range(3):\n",
    "    sample = run_walk_dataset[i]\n",
    "    print()\n",
    "    print(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders for Train/Test Datasets\n",
    "\n",
    "DataLoaders allow you to do things like batch data, shuffle data, etc.â€”they are the standard way to iterate through data for training a PyTorch model.\n",
    "\n",
    "The below code also _randomly_ splits the single, loaded RunWalkDataset into separate train and test datasets. \n",
    "\n",
    "> **TASK**: Critique Andy's method for splitting this data (you do not need to change the code).\n",
    "\n",
    "> In a sentence or two, describe one thing about the below split is good practice and one thing that is not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: Double-click to edit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# split data into train and test sets randomly ~ 80/20\n",
    "\n",
    "# lengths or # samples in each dataset\n",
    "split_80 = len(run_walk_dataset)*80//100\n",
    "split_20 = len(run_walk_dataset) - split_80\n",
    "\n",
    "# random split\n",
    "train_dataset, test_dataset = random_split(run_walk_dataset, [split_80, split_20])\n",
    "\n",
    "# how many samples per batch\n",
    "batch_size = 64\n",
    "\n",
    "# train and test loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Define the Neural Network Architecture\n",
    "\n",
    "The architecture should be responsible for transforming input features into a single target class value between 0-1. \n",
    "\n",
    "> **TASK**: Something is missing from Andy's code, find out what it is and fix it so that you can train and calculate test metrics on this model.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing NN modules\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## TODO: Fix Andy's code\n",
    "\n",
    "class AndyNet(nn.Module):\n",
    "    \n",
    "    ## Defines a single-layer NN\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        '''Defines layers of a neural network.\n",
    "           :param input_dim: Number of input features\n",
    "           :param output_dim: Number of outputs\n",
    "         '''\n",
    "        super(AndyNet, self).__init__()\n",
    "                \n",
    "        # define a linear layer, input > output\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    \n",
    "    ## Defines the feedforward behavior of the network\n",
    "    def forward(self, x):\n",
    "        '''Feedforward behavior of the net.\n",
    "           :param x: A batch of input features\n",
    "           :return: A batch of output values; predictions\n",
    "         '''\n",
    "        out = self.fc1(x)\n",
    "        out = self.sigmoid(out) # final output, activation fn to get class probs \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the simple NN with specified dimensions\n",
    "\n",
    "input_dim = 7 # input feats\n",
    "output_dim = 1 # one target value\n",
    "\n",
    "model = AndyNet(input_dim, output_dim)\n",
    "\n",
    "# print model layers (from init fn)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss and optimization strategy\n",
    "\n",
    "The loss function defines what a network tries to minimize in terms of comparing actual versus predicted values. \n",
    "\n",
    "In classification tasks, it is common to use a **cross entropy loss**; here since there is only one value output by the modelâ€”a value between 0-1â€”there is a special *binary* cross entropy loss, `BCELoss`. \n",
    "\n",
    "The optimizer defines how a neural network's weights update, as a result of trying to minimize the loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function (categorical cross-entropy for classification)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Train Loop\n",
    "\n",
    "In the `helpers.py` file, there is code for a training loop that does the following:\n",
    "* Iterate through the training data in batches provided by the `train_loader` \n",
    "* Calculate the loss (binary cross-entropy) and backpropagate to find the source of this error\n",
    "* Update the weights of this NN to decrease the loss\n",
    "* Return the trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import train\n",
    "\n",
    "# number of epochs - times you iterate through the entire training dataset\n",
    "n_epochs = 5\n",
    "\n",
    "# call provided train function with all params\n",
    "model = train(model, train_loader, n_epochs, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test the Trained Network\n",
    "\n",
    "> **TASK**: Record the accuracy that Andy's network gets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Andy's test accuracy was**: Your answer here (double-click to edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import test_eval\n",
    "\n",
    "# calculate test accuracy with helper function\n",
    "num_correct = test_eval(model, test_loader, criterion)\n",
    "\n",
    "print('Test accuracy: {:.6f}\\n'.format(num_correct/len(test_dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## An Improved NN\n",
    "\n",
    "Now it's your turn to improve upon this code. \n",
    "\n",
    "> **TASK**: Using the *Sequential* module, create a new NN class that improves upon Andy's solution. For at least 2 experiments, record:\n",
    ">* Hypothesis: What you think will improve a model's accuracy and why (e.g., changing the number of nodes in a hidden layer)\n",
    ">* Experiment results: The resultant test accuracy\n",
    "\n",
    "ðŸ† Your final experiment should aim for about **98% test accuracy**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Define, train, and test your own NN, using the Sequential module\n",
    "\n",
    "## TODO, note in markdown or cell comments, which model choices seemed to work best (# epochs, layers ,etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Room for your experiment notes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un-mount your data\n",
    "\n",
    "When you're totally done with the Trove dataset, un-mount it to clean uop this working directory.\n",
    "\n",
    "> **TASK**: Un-mount the run/walk Trove data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Un-mount Trove data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "---\n",
    "# Further UX Considerations ðŸ“ \n",
    "\n",
    "At Apple, we are always thinking about the nuances of the user experience for different populations. This section represent answers to a set of questions that ask us to consider inclusive design practices, such as:\n",
    "\n",
    "* **Failure cases**: What might go wrong with activity classification, and how does the likelihood of failures vary across users?\n",
    "* **Delight**: What potential impact of a run/walk detection feature are you most excited about?\n",
    "\n",
    "For any model you are thinking of putting into production or sharing with a larger team, you should critically consider the different tradeoffs and impacts such a trained model could have on different users. \n",
    "\n",
    "> **TASK**: In a sentence or a short bullet point, write down at least one potential failure case for run/walk detection and the user impact of that failure. \n",
    "\n",
    "> Additionally, share whether or not you would release your model more widely considering how _big_ this dataset, how many users it represents, and what data might be useful that is missing from this data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
